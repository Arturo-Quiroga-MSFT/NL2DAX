# üíª CODE Directory

This directory contains the core application code for the NL2DAX system. Each module has a specific responsibility in the natural language to DAX/SQL pipeline.

## üìã Overview

The CODE directory implements a modular architecture for converting natural language queries into executable DAX and SQL queries, with comprehensive diagnostic and testing capabilities.

## üóÇÔ∏è File Structure

| File | Purpose | Key Functions |
|------|---------|---------------|
| `main.py` | **Main pipeline** | Entry point, orchestrates entire flow |
| `dax_generator.py` | **DAX generation** | Converts NL to DAX using Azure OpenAI |
| `sql_executor.py` | **SQL execution** | Executes SQL queries against Azure SQL |
| `schema_reader.py` | **Schema awareness** | Reads database schema and relationships |
| `dax_formatter.py` | **DAX formatting** | Formats DAX using external API |
| `query_executor.py` | **Query coordination** | Coordinates DAX and SQL generation |
| `db_connection_check.py` | **Database testing** | Tests Azure SQL connectivity |
| `diagnose_permissions.py` | **Power BI diagnostics** | Comprehensive service principal testing |
| `xmla_status_check.py` | **XMLA testing** | Quick XMLA endpoint status check |
| `smoke_test_xmla.py` | **Simple XMLA test** | Basic XMLA connectivity verification |
| `test_permissions.py` | **Permission validation** | Tests various Power BI permissions |
| `troubleshoot_permissions.py` | **Advanced diagnostics** | Deep permission troubleshooting |
| `xmla_http_executor.py` | **XMLA HTTP client** | HTTP-based XMLA query execution |
| `add_sp_to_dataset.py` | **SP management** | Adds service principal to datasets |

## üöÄ Core Modules

### main.py
The primary entry point that orchestrates the entire NL2DAX pipeline.

**Key Features:**
- Interactive command-line interface
- Natural language question processing
- Dual output generation (DAX + SQL)
- Result formatting with ASCII banners
- Timestamped result logging

**Usage:**
```bash
python main.py
```

### dax_generator.py
Converts natural language into DAX queries using Azure OpenAI.

**Key Features:**
- Schema-aware DAX generation
- Context injection from database schema
- Error handling and validation
- Integration with LangChain

**Dependencies:**
- Azure OpenAI
- Schema cache (schema_cache.json)

### sql_executor.py
Handles SQL query execution against Azure SQL Database.

**Key Features:**
- ODBC connection management
- Query sanitization and extraction
- Tabular result formatting
- Comprehensive error handling

**Configuration:**
Requires environment variables for SQL Server connection.

### schema_reader.py
Reads and caches database schema information.

**Key Features:**
- Automatic schema discovery
- Relationship mapping
- Primary key identification
- JSON caching for performance
- Command-line cache refresh

**Usage:**
```bash
# View schema summary
python schema_reader.py

# Refresh schema cache
python schema_reader.py --cache
```

## üîß Diagnostic Tools

### diagnose_permissions.py
Comprehensive diagnostic tool for Power BI service principal permissions.

**Tests Performed:**
- Service principal authentication
- Workspace access verification
- Dataset permission validation
- DAX query execution testing

**Usage:**
```bash
python diagnose_permissions.py
```

### xmla_status_check.py
Quick status checker for XMLA endpoint availability.

**Features:**
- Simple DAX execution test
- Clear pass/fail status reporting
- Minimal dependencies

### db_connection_check.py
Tests Azure SQL Database connectivity.

**Validates:**
- Connection string configuration
- Authentication credentials
- Network connectivity
- Database accessibility

## üìÅ Support Files

### schema_cache.json
JSON cache file containing database schema information.

**Contains:**
- Table definitions
- Column metadata
- Relationship mappings
- Primary key information

**Auto-generated by:** `schema_reader.py`

### requirements.txt
Python dependency specification for the CODE directory.

**Key Dependencies:**
- `azure-identity` - Azure authentication
- `pyodbc` - SQL Server connectivity
- `python-dotenv` - Environment management
- `azure-openai` - Azure OpenAI API
- `langchain-openai` - LangChain integration
- `tabulate` - Table formatting
- `requests` - HTTP requests

## üîê Configuration

### Environment Variables (.env)

Create a `.env` file in this directory with:

```bash
# Azure OpenAI
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your-key
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-mini
AZURE_OPENAI_API_VERSION=2023-05-15

# Azure SQL Database
SQL_SERVER=your-server.database.windows.net
SQL_DATABASE=your-database
SQL_USERNAME=your-username
SQL_PASSWORD=your-password
ODBC_DRIVER=ODBC Driver 18 for SQL Server

# Power BI (for DAX execution)
POWER_BI_CLIENT_ID=your-service-principal-client-id
POWER_BI_CLIENT_SECRET=your-service-principal-secret
POWER_BI_TENANT_ID=your-tenant-id
POWER_BI_WORKSPACE_ID=your-workspace-id
POWER_BI_DATASET_ID=your-dataset-id
```

## üèÉ‚Äç‚ôÇÔ∏è Quick Start

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Configure environment:**
   ```bash
   cp .env.example .env
   # Edit .env with your credentials
   ```

3. **Test connectivity:**
   ```bash
   python db_connection_check.py
   python diagnose_permissions.py
   ```

4. **Run the main pipeline:**
   ```bash
   python main.py
   ```

## üîç Troubleshooting

### Common Issues

1. **Module Import Errors**
   - Ensure you're in the CODE directory when running scripts
   - Verify all dependencies are installed: `pip install -r requirements.txt`

2. **Database Connection Failures**
   - Run `db_connection_check.py` to diagnose
   - Verify SQL Server credentials in `.env`
   - Check firewall settings

3. **Power BI Authentication Issues**
   - Run `diagnose_permissions.py` for detailed analysis
   - Verify service principal setup
   - Check XMLA endpoint configuration

4. **Schema Cache Issues**
   - Delete `schema_cache.json` and run `schema_reader.py --cache`
   - Verify database permissions for schema queries

### Debug Mode

Most scripts support verbose logging. Set environment variable:
```bash
export LOG_LEVEL=DEBUG
```

## üß™ Testing

### Unit Tests
```bash
# Run individual module tests
python -m pytest test_*.py
```

### Integration Tests
```bash
# Test end-to-end functionality
python main.py
# Enter: "List top 5 customers by balance"
```

### Smoke Tests
```bash
# Quick connectivity tests
python smoke_test_xmla.py
python db_connection_check.py
```

## üìä Output Files

All query results are automatically saved to `../RESULTS/` with timestamps:
- Format: `nl2dax_run_{sanitized_question}_{timestamp}.txt`
- Contains: Original question, generated DAX, generated SQL, execution results
- Retention: Manual cleanup required

## üîÑ Development Workflow

1. **Make changes** to core modules
2. **Test connectivity** with diagnostic tools
3. **Run integration tests** via `main.py`
4. **Verify outputs** in RESULTS directory
5. **Update schema cache** if database changes

---

üí° **Tip**: Always test with the diagnostic tools after making configuration changes to ensure everything is working correctly.
